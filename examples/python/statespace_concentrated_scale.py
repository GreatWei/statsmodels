# coding: utf-8

# DO NOT EDIT
# Autogenerated from the notebook statespace_concentrated_scale.ipynb.
# Edit the notebook and then sync the output with this file.
#
# flake8: noqa
# DO NOT EDIT

# ## 状态空间模型 - 比例浓缩在似然函数之外

import numpy as np
import pandas as pd
import statsmodels.api as sm

dta = sm.datasets.macrodata.load_pandas().data
dta.index = pd.PeriodIndex(start='1959Q1', end='2009Q3', freq='Q')

# ### 介绍
#
# (其中大多是基于 Harvey (1989)， 请参阅第 3.4 节)
#
# 状态空间模型一般可编写如下 (我们关注的是时间固定的状态空间模型，但类似的结果适用于时间可变模型):
#
# $$
# \begin{align}
# y_t & = Z \alpha_t + \varepsilon_t, \quad \varepsilon_t \sim N(0, H) \\
# \alpha_{t+1} & = T \alpha_t + R \eta_t \quad \eta_t \sim N(0, Q)
# \end{align}
# $$
#
# 通常，矩阵 $Z, H, T, R, Q$ 中的某些或全部值是未知的，必须进行估计； 在统计模型中，估计通常是通过找到
# 极大似然函数的参数来完成的。 特别是，如果我们将参数汇集在向量 $\psi$ 中，则可以每个矩阵视为函数的参数，
# 例如 $Z = Z(\psi)$ 等。
#
# 通常，似然函数是数值上的最大化，例如通过应用拟牛顿"爬山"算法，随着参数的增加，变得越来越困难。 事实证明，
# 在许多情况下，我们可以将模型重新参数化为 $[\psi_*', \sigma_*^2]'$ ，其中 $\sigma_*^2$ 是模型的“比例”
# （通常替换为 误差方差项之一），并且可以通过对似然函数求微分来解析地找到 $\sigma_*^2$ 的极大似然估计。 
# 这意味着仅需要数值方法来估计参数 $\psi_*$，该参数的维数比 $\psi_*$ 小一。


# ### 示例: local level model
#
# (请参见示例，Harvey (1989) 的第 4.2 节)
#
# 作为一个具体示例，请考虑本地模型，可以将它写作:
#
# $$
# \begin{align}
# y_t & = \alpha_t + \varepsilon_t, \quad \varepsilon_t \sim N(0,
# \sigma_\varepsilon^2) \\
# \alpha_{t+1} & = \alpha_t + \eta_t \quad \eta_t \sim N(0, \sigma_\eta^2)
# \end{align}
# $$
#
# 在这个模型中, $Z, T,$ 和 $R$ 均固定等于 $1$, 且有两个未知参数，所以 $\psi = [\sigma_\varepsilon^2,\sigma_\eta^2]$。

# #### 经典方法
#
# 首先，我们展示如何使用 statsmodels 的状态空间库来定义此模型，无需浓缩比例：


class LocalLevel(sm.tsa.statespace.MLEModel):
    _start_params = [1., 1.]
    _param_names = ['var.level', 'var.irregular']

    def __init__(self, endog):
        super(LocalLevel, self).__init__(
            endog, k_states=1, initialization='diffuse')

        self['design', 0, 0] = 1
        self['transition', 0, 0] = 1
        self['selection', 0, 0] = 1

    def transform_params(self, unconstrained):
        return unconstrained**2

    def untransform_params(self, unconstrained):
        return unconstrained**0.5

    def update(self, params, **kwargs):
        params = super(LocalLevel, self).update(params, **kwargs)

        self['state_cov', 0, 0] = params[0]
        self['obs_cov', 0, 0] = params[1]


# 在这个模型中必须选择两个参数: `var.level` $(\sigma_\eta^2)$ 和 `var.irregular` $(\sigma_\varepsilon^2)$. 
# 我们可以通过数值最大化似然函数内置的 `fit` 方法来选择两个参数。
#
# 在我们的示例中，我们将本地模型应用于消费者物价指数通胀。

mod = LocalLevel(dta.infl)
res = mod.fit(disp=False)
print(res.summary())

# 我们可以通过结果属性 `mle_retvals` 来查看数值优化器的结果：

print(res.mle_retvals)

# #### 浓缩比例

# 现在，有两种方法可以将上面的模型重新参数化:
#
# 1. 第一种方法是设置 $\sigma_*^2 \equiv \sigma_\varepsilon^2$ ，来使 $\psi_* = \psi / \sigma_\varepsilon^2 = [1, q_\eta]$ 其中 $q_\eta = \sigma_\eta^2 / \sigma_\varepsilon^2$.
# 2. 第二种方法是设置t $\sigma_*^2 \equiv \sigma_\eta^2$ 来使 $\psi_* = \psi / \sigma_\eta^2 = [h, 1]$ 其中 $h = \sigma_\varepsilon^2 / \sigma_\eta^2$.
#
# 在第一种情况下，我们只需要在数值上最大化关于 $q_\eta$ 的似然，在第二种情况下，我们只需要在数值上最大化关于 $h$ 的似然。
#
# 两种方法在大多数情况下都可以很好地运行，在下面的示例中，我们将使用第二种方法。

# 重新构建模型以便于似然函数收敛，我们需要将参数向量 $\psi_* = [g, 1]$ 写入模型。 因为这个参数向量定义了$ \ sigma_ \ eta ^ 2 \ equiv 1 $，
# 所以现在我们囊括一条新线 `self ['state_cov'，0，0] = 1`，且 $h$ 是唯一未知的参数。 因为我们的参数 $h$ 不是方差，所以将它重命名为 `ratio.irregular`。
# 
#
# 建立模型以便从Kalman过滤器递归（而不是从数字上选择）中计算比例所需的关键是设置“ self.ssm.filter_concentrated = True”。



class LocalLevelConcentrated(sm.tsa.statespace.MLEModel):
    _start_params = [1.]
    _param_names = ['ratio.irregular']

    def __init__(self, endog):
        super(LocalLevelConcentrated, self).__init__(
            endog, k_states=1, initialization='diffuse')

        self['design', 0, 0] = 1
        self['transition', 0, 0] = 1
        self['selection', 0, 0] = 1
        self['state_cov', 0, 0] = 1

        self.ssm.filter_concentrated = True

    def transform_params(self, unconstrained):
        return unconstrained**2

    def untransform_params(self, unconstrained):
        return unconstrained**0.5

    def update(self, params, **kwargs):
        params = super(LocalLevelConcentrated, self).update(params, **kwargs)
        self['obs_cov', 0, 0] = params[0]


# 同样，我们可以使用内置的 `fit` 方法来找到 $h$ 的极大似然估计。


mod_conc = LocalLevelConcentrated(dta.infl)
res_conc = mod_conc.fit(disp=False)
print(res_conc.summary())

# 在参数的中间表(`ratio.irregular`)中提供了 $h$ 的估计值，而上表提供比例估计值。下面我们将展示这些估计与以往方法的估计一致。


# 而且我们可以再次从结果属性 `mle_retvals` 中查看数值优化的结果，事实证明，在这个示例中只需选择较少的参数，因此仅需进行两次迭代。
# 此外，由于数值最大化问题更容易，因此优化器能够找到一个值，该值使得该参数的梯度比上面的梯度更加接近零。

print(res_conc.mle_retvals)

# #### 比较估计
#
# 重新调用 $h = \sigma_\varepsilon^2 / \sigma_\eta^2$ 且比例是 $\sigma_*^2 = \sigma_\eta^2$. 使用这些定义，我们可以看到两个模型产生的结果几乎相同：
print('Original model')
print('var.level     = %.5f' % res.params[0])
print('var.irregular = %.5f' % res.params[1])

print('\nConcentrated model')
print('scale         = %.5f' % res_conc.scale)
print('h * scale     = %.5f' % (res_conc.params[0] * res_conc.scale))

# ### 示例: SARIMAX
#
# SARIMAX 模型的默认设置, 通过对似然函数的数值最大化来选择方差项，增加了一个比例浓缩选项。

# 经典方法
mod_ar = sm.tsa.SARIMAX(dta.cpi, order=(1, 0, 0), trend='ct')
res_ar = mod_ar.fit(disp=False)

# 比例浓缩的估计模型
mod_ar_conc = sm.tsa.SARIMAX(
    dta.cpi, order=(1, 0, 0), trend='ct', concentrate_scale=True)
res_ar_conc = mod_ar_conc.fit(disp=False)

# 这两种方法产生了相同的对数似然和参数，但浓缩比例的模型能够稍微改善拟合度：

print('Loglikelihood')
print('- Original model:     %.4f' % res_ar.llf)
print('- Concentrated model: %.4f' % res_ar_conc.llf)

print('\nParameters')
print('- Original model:     %.4f, %.4f, %.4f, %.4f' % tuple(res_ar.params))
print('- Concentrated model: %.4f, %.4f, %.4f, %.4f' %
      (tuple(res_ar_conc.params) + (res_ar_conc.scale, )))

# 此时，浓缩方法使得优化器的迭代次数减少了约 1/3：

print('Optimizer iterations')
print('- Original model:     %d' % res_ar.mle_retvals['iterations'])
print('- Concentrated model: %d' % res_ar_conc.mle_retvals['iterations'])
